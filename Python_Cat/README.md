# main.py

The code is divided into sections according to the operations performed. It starts by importing all necessary modules for functionality: `os`, `sys`, and three files containing various classes implemented in the main code (`DataPreparation.py`, `Classifier.py`, `MetricPrinter.py`), which are imported as modules (`dl`, `clf`, `mp` respectively). Following this, there is a check for file execution, primarily for cases where the user wants to execute `main.py` without using the bash file. In such a case, the user will be prompted to type `python3 main.py <model_name>`. Once the model is declared among the proposed ones (see bash file), the dataset is loaded, and various sections commence:

- **Preparing Data:** The `DataPreparation` class (in `DataPreparation.py`) loads and normalizes the data. Additionally, the data is split into training and test sets and further divided into two groups based on a specific characteristic to create two different categories; the latter operation is done accordingly to the value of a spectator variable, "eta", not used in the train.
- **Model Definition:** The `model_type` value is checked to specify the type of model to train and evaluate. Depending on the selected model type, an instance of the `SignalBackgroundClassifier` class is instantiated with the specified model type and training data considering the `entire dataset`, categories `cat1` and `cat2`. Supported models include BDT, Neural Network, Random Forest, SVT, and kNN. For each model selected, three classifiers with different iperparameters are defined, one for each categories; the better iperparameters are obtained performing a grid search.
- **Training & Evaluation:** After instantiating the classifier, the `train_classifier()` method is called to train the model using training data. This method accepts training data for the `entire dataset`, categories `cat1` and `cat2`, as well as their respective labels. The time taken for training on full dataset is recorded. Subsequently, the `evaluate_classifier()` method is called to evaluate the model's performance using testing data. This method computes various evaluation metrics such as accuracy, F1-score, precision, and ROC curve. It utilizes testing data corresponding to the `entire dataset` and categories `cat1` and `cat2` for evaluation. Additional evaluations such as feature importance are also performed.
 - **Print Results:** Results are written to a text file including evaluation metrics and results of the false positive rate `fpr` and the true positive rate `tpr` for subsequently drawing the comparative ROC curves among the models selected.
- **Metrics Display:** An instance of the `PrintMetrics` class is created, passing various metrics calculated during model evaluation as arguments. These metrics include `fpr` and `tpr` of the entire dataset, combined false positive rates and true positive rates (`fpr_combined` and `tpr_combined`) from categories, accuracy (`accuracy`), F1-score (`f1`), and precision (`precision`), as well as combined accuracy (`accuracy_combined`), combined F1-score (`f1_combined`), and combined precision (`precision_combined`). It also includes `y_test`, `y_test_combined`, `predictions`, and `predictions_combined` to print the confusion matrix. The `plot_roc_curve()` method of the `PrintMetrics` instance is called to visualize the ROC curve. This method creates a graph showing the relationship between signal efficiency (True Positive Rate) and background rejection (1 - False Positive Rate) for both the single and combined datasets. The `print_metrics()` method of the `PrintMetrics` instance is called to print the model's metrics, including accuracy, F1-score, and precision.
